{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "data_path = \"/opt/ml/data\"\n",
    "\n",
    "train_path = os.path.join(data_path, \"train\")\n",
    "val_path = os.path.join(data_path, \"val\")\n",
    "test_path = os.path.join(data_path, \"test\")\n",
    "\n",
    "\n",
    "# DATA_PATH: \"/opt/ml/data/\"\n",
    "# DATASET: \"TACO\"\n",
    "# IMG_SIZE: 224\n",
    "# AUG_TRAIN: \"simple_augment_train\"\n",
    "# AUG_TEST: \"simple_augment_test\"\n",
    "# AUG_TRAIN_PARAMS: \n",
    "# BATCH_SIZE: 64\n",
    "# EPOCHS: 100\n",
    "# VAL_RATIO: 0.0\n",
    "# INIT_LR: 0.1\n",
    "# FP16: True\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = get_dataset(\n",
    "#         data_path=config[\"DATA_PATH\"],\n",
    "#         dataset_name=config[\"DATASET\"],\n",
    "#         img_size=config[\"IMG_SIZE\"],\n",
    "#         val_ratio=config[\"VAL_RATIO\"],\n",
    "#         transform_train=config[\"AUG_TRAIN\"],\n",
    "#         transform_test=config[\"AUG_TEST\"],\n",
    "#         transform_train_params=config[\"AUG_TRAIN_PARAMS\"],\n",
    "#         transform_test_params=config.get(\"AUG_TEST_PARAMS\"),\n",
    "#     )\n",
    "\n",
    "transform_train = \"simple_augment_train\"\n",
    "transform_test = \"simple_augment_test\"\n",
    "dataset_name = \"TACO\"\n",
    "img_size = 224\n",
    "transform_train_params = dict()\n",
    "transform_test_params = dict()\n",
    "\n",
    " # preprocessing policies\n",
    "transform_train = getattr(\n",
    "    __import__(\"src.augmentation.policies\", fromlist=[\"\"]),\n",
    "    transform_train,\n",
    ")(dataset=dataset_name, img_size=img_size, **transform_train_params)\n",
    "\n",
    "\n",
    "transform_test = getattr(\n",
    "    __import__(\"src.augmentation.policies\", fromlist=[\"\"]),\n",
    "    transform_test,\n",
    ")(dataset=dataset_name, img_size=img_size, **transform_test_params)\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root=train_path, transform=transform_train)\n",
    "val_dataset = ImageFolder(root=val_path, transform=transform_test)\n",
    "test_dataset = ImageFolder(root=test_path, transform=transform_test)\n",
    "\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl, val_dl, test_dl = create_dataloader(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer = TorchTrainer(\n",
    "    model=model_instance,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    model_path=model_path,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "test_loss, test_f1, test_acc = trainer.test(\n",
    "        model=model_instance, test_dataloader=val_dl if val_dl else test_dl\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "model_instance = torchvision.models.mobilenet_v3_large(pretrained=True, width_mult=1.0,  reduced_tail=False, dilated=False)\n",
    "\n",
    "model_instance.load_state_dict(\n",
    "            torch.load(\"/opt/ml/code/exp/latest2/best.pt\")\n",
    "        )\n",
    "\n",
    "save_path = os.path.join(\"/opt/ml/code/model\", \"mobilenet_v3_best.pt\")\n",
    "torch.save(model_instance, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.models.mobilenetv3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11577/379633544.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/opt/ml/code/model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mobilenet_v3_best.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eval/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/eval/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.models.mobilenetv3'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "save_path = os.path.join(\"/opt/ml/code/model\", \"mobilenet_v3_best.pt\")\n",
    "\n",
    "model = torch.load(save_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "args.start_epoch = checkpoint['epoch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = torchvision.models.quantization.__dict__[\"mobilenet_v3_large\"](pretrained=True)#, quantize=args.test_only)\n",
    "model.to(device)\n",
    "\n",
    "model.fuse_model()\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig(\"qnnpack\")\n",
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"/opt/ml/code/mobile_q/model_1.pth\", map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizableMobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNActivation(\n",
       "      (0): ConvBn2d(\n",
       "        3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (weight_fake_quant): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0683], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.443112373352051, max_val=8.70357894897461)\n",
       "        )\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6814], device='cuda:0'), zero_point=tensor([125], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-85.1651840209961, max_val=88.6010971069336)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): Hardswish(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3254], device='cuda:0'), zero_point=tensor([1], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36723437905311584, max_val=82.6009521484375)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0251], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0526463985443115, max_val=3.2063560485839844)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0623], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.895705223083496)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0298], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.798401117324829, max_val=2.95528507232666)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3805], device='cuda:0'), zero_point=tensor([146], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-55.492591857910156, max_val=41.54016876220703)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2856], device='cuda:0'), zero_point=tensor([76], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-21.581979751586914, max_val=51.248443603515625)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7471927404403687, max_val=0.8891363143920898)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1056], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=26.9316349029541)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False\n",
       "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0384], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.8022756576538086, max_val=4.893355846405029)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0524], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.358247756958008)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0098], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.209701657295227, max_val=1.2466069459915161)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0763], device='cuda:0'), zero_point=tensor([135], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.333648681640625, max_val=9.118569374084473)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5215381979942322, max_val=0.783511757850647)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0233], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.953510284423828)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False\n",
       "            (bn): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0437], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.577762126922607, max_val=5.182235240936279)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0384], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.795835494995117)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0204], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5794607400894165, max_val=2.6060893535614014)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1008], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.915051460266113, max_val=12.794621467590332)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1059], device='cuda:0'), zero_point=tensor([131], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.851588249206543, max_val=13.159311294555664)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7566700577735901, max_val=0.8354728817939758)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0332], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=8.459089279174805)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False\n",
       "            (bn): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2562603950500488, max_val=1.1812547445297241)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0241], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.133589744567871)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizableSqueezeExcitation(\n",
       "          (fc1): ConvReLU2d(\n",
       "            72, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0020], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.056989990174770355, max_val=0.25178080797195435)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.711935997009277)\n",
       "            )\n",
       "          )\n",
       "          (relu): Identity()\n",
       "          (fc2): Conv2d(\n",
       "            24, 72, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0013], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.645457517606701e-09, max_val=0.16573941707611084)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0129], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4749421684001618e-09, max_val=3.294492721557617)\n",
       "            )\n",
       "          )\n",
       "          (skip_mul): FloatFunctional(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0230], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.866636753082275)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0151], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.59378182888031, max_val=1.92647385597229)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0726], device='cuda:0'), zero_point=tensor([142], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.34050178527832, max_val=8.16966724395752)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0034], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.436687171459198, max_val=0.4014977812767029)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0117], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.990647315979004)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False\n",
       "            (bn): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0931], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.800044059753418, max_val=11.867207527160645)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0618], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.750110626220703)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizableSqueezeExcitation(\n",
       "          (fc1): ConvReLU2d(\n",
       "            120, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0120], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.464240550994873, max_val=1.5244628190994263)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0160], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.089986801147461)\n",
       "            )\n",
       "          )\n",
       "          (relu): Identity()\n",
       "          (fc2): Conv2d(\n",
       "            32, 120, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0146], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8568761348724365, max_val=1.0975370407104492)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0362], device='cuda:0'), zero_point=tensor([146], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.293428421020508, max_val=3.940809726715088)\n",
       "            )\n",
       "          )\n",
       "          (skip_mul): FloatFunctional(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0180], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=4.602747440338135)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0217], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.772618293762207, max_val=2.364391803741455)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0464], device='cuda:0'), zero_point=tensor([142], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.611303329467773, max_val=5.2248454093933105)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0664], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.476192474365234, max_val=8.446379661560059)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0025], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31561365723609924, max_val=0.24408018589019775)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.08354115486145)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBnReLU2d(\n",
       "            120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False\n",
       "            (bn): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0638], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.132133483886719, max_val=8.057909965515137)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0664], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.92418670654297)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): QuantizableSqueezeExcitation(\n",
       "          (fc1): ConvReLU2d(\n",
       "            120, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0151], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0968995094299316, max_val=1.9229459762573242)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0119], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.0250511169433594)\n",
       "            )\n",
       "          )\n",
       "          (relu): Identity()\n",
       "          (fc2): Conv2d(\n",
       "            32, 120, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0159], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1710251569747925, max_val=2.0253047943115234)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0327], device='cuda:0'), zero_point=tensor([134], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.363749027252197, max_val=3.967857837677002)\n",
       "            )\n",
       "          )\n",
       "          (skip_mul): FloatFunctional(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0355], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=9.057405471801758)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0212], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.477651834487915, max_val=2.700326442718506)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0681], device='cuda:0'), zero_point=tensor([126], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.584551811218262, max_val=8.79287338256836)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0903], device='cuda:0'), zero_point=tensor([134], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.133462905883789, max_val=10.894457817077637)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0055], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7069688439369202, max_val=0.6813898086547852)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0768], device='cuda:0'), zero_point=tensor([136], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.485466957092285, max_val=9.106202125549316)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0361], device='cuda:0'), zero_point=tensor([10], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37487396597862244, max_val=8.841114044189453)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False\n",
       "            (bn): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0650], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.400545120239258, max_val=8.283839225769043)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0833], device='cuda:0'), zero_point=tensor([156], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.029611587524414, max_val=8.21300220489502)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0323], device='cuda:0'), zero_point=tensor([12], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37492820620536804, max_val=7.86810302734375)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0058], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6993168592453003, max_val=0.7372931241989136)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0445], device='cuda:0'), zero_point=tensor([134], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.966484069824219, max_val=5.375148296356201)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0030], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3882157504558563, max_val=0.36530089378356934)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0312], device='cuda:0'), zero_point=tensor([166], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.182956218719482, max_val=2.772305727005005)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0115], device='cuda:0'), zero_point=tensor([33], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3749910593032837, max_val=2.5632123947143555)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False\n",
       "            (bn): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0773], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.853141784667969, max_val=9.717680931091309)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0324], device='cuda:0'), zero_point=tensor([173], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.6106181144714355, max_val=2.6517233848571777)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0104], device='cuda:0'), zero_point=tensor([36], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37498414516448975, max_val=2.287478446960449)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0256], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7305119037628174, max_val=3.2638587951660156)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0373], device='cuda:0'), zero_point=tensor([113], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.228089332580566, max_val=5.29403829574585)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0432], device='cuda:0'), zero_point=tensor([132], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.723133087158203, max_val=5.300427436828613)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0035], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3884017765522003, max_val=0.44668665528297424)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0387], device='cuda:0'), zero_point=tensor([148], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.710482597351074, max_val=4.150995254516602)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0173], device='cuda:0'), zero_point=tensor([22], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37497925758361816, max_val=4.030940055847168)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False\n",
       "            (bn): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0698], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.829620361328125, max_val=8.899365425109863)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0323], device='cuda:0'), zero_point=tensor([173], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.593672752380371, max_val=2.6470398902893066)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([38], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.374980628490448, max_val=2.155717372894287)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.095367431640625, max_val=2.856034994125366)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0528], device='cuda:0'), zero_point=tensor([110], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.818845748901367, max_val=7.633813381195068)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0692], device='cuda:0'), zero_point=tensor([113], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.840390682220459, max_val=9.80703353881836)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8267269134521484, max_val=0.5890986919403076)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0816], device='cuda:0'), zero_point=tensor([126], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.288741111755371, max_val=10.509146690368652)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0414], device='cuda:0'), zero_point=tensor([9], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3748222291469574, max_val=10.188556671142578)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False\n",
       "            (bn): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1077], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.72933578491211, max_val=8.435074806213379)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0523], device='cuda:0'), zero_point=tensor([141], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.379382610321045, max_val=5.957286357879639)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0208], device='cuda:0'), zero_point=tensor([18], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37497425079345703, max_val=4.919093132019043)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0183], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.3291046619415283, max_val=1.7567893266677856)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0516], device='cuda:0'), zero_point=tensor([140], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.200753688812256, max_val=5.95153284072876)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0504], device='cuda:0'), zero_point=tensor([123], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.186618804931641, max_val=6.666121482849121)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8418020606040955, max_val=0.6696239113807678)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0624], device='cuda:0'), zero_point=tensor([126], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.875866413116455, max_val=8.036137580871582)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0318], device='cuda:0'), zero_point=tensor([12], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37494730949401855, max_val=7.730172634124756)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "            (bn): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1506], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.20072364807129, max_val=14.33332347869873)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1569], device='cuda:0'), zero_point=tensor([63], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.857582092285156, max_val=30.160057067871094)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1110], device='cuda:0'), zero_point=tensor([3], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37469765543937683, max_val=27.942413330078125)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): QuantizableSqueezeExcitation(\n",
       "          (fc1): ConvReLU2d(\n",
       "            480, 120, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0153], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9451985359191895, max_val=1.7568706274032593)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0124], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.163339853286743)\n",
       "            )\n",
       "          )\n",
       "          (relu): Identity()\n",
       "          (fc2): Conv2d(\n",
       "            120, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0116], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9356802701950073, max_val=1.4755948781967163)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0330], device='cuda:0'), zero_point=tensor([140], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.597388744354248, max_val=3.805703639984131)\n",
       "            )\n",
       "          )\n",
       "          (skip_mul): FloatFunctional(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0246], device='cuda:0'), zero_point=tensor([14], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3466106951236725, max_val=5.929959297180176)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0098], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9450785517692566, max_val=1.2478581666946411)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0364], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.668808460235596, max_val=4.625734329223633)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5332755446434021, max_val=0.5013412237167358)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0532], device='cuda:0'), zero_point=tensor([140], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.441830158233643, max_val=6.114478588104248)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0248], device='cuda:0'), zero_point=tensor([15], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3749827444553375, max_val=5.954484939575195)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False\n",
       "            (bn): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1044], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.314440727233887, max_val=11.48089599609375)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1520], device='cuda:0'), zero_point=tensor([82], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.533498764038086, max_val=26.227930068969727)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0998], device='cuda:0'), zero_point=tensor([4], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3746050298213959, max_val=25.06743812561035)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): QuantizableSqueezeExcitation(\n",
       "          (fc1): ConvReLU2d(\n",
       "            672, 168, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0110], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4071667194366455, max_val=1.2506589889526367)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=2.5273959636688232)\n",
       "            )\n",
       "          )\n",
       "          (relu): Identity()\n",
       "          (fc2): Conv2d(\n",
       "            168, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0196], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3169183731079102, max_val=2.501776695251465)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0420], device='cuda:0'), zero_point=tensor([171], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.172817230224609, max_val=3.526278495788574)\n",
       "            )\n",
       "          )\n",
       "          (skip_mul): FloatFunctional(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([41], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.40810665488243103, max_val=2.125178337097168)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0177], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.256040096282959, max_val=2.12004017829895)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0382], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.857102394104004, max_val=4.878654956817627)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0471], device='cuda:0'), zero_point=tensor([123], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.7933268547058105, max_val=6.204914093017578)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6874971389770508, max_val=0.6581409573554993)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0692], device='cuda:0'), zero_point=tensor([120], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.324413299560547, max_val=9.326313972473145)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0367], device='cuda:0'), zero_point=tensor([10], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37493398785591125, max_val=8.972921371459961)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False\n",
       "            (bn): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0471], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.011296272277832, max_val=5.78935432434082)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0774], device='cuda:0'), zero_point=tensor([108], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.371827125549316, max_val=11.356270790100098)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0442], device='cuda:0'), zero_point=tensor([8], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37497007846832275, max_val=10.899833679199219)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): QuantizableSqueezeExcitation(\n",
       "          (fc1): ConvReLU2d(\n",
       "            672, 168, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4194892644882202, max_val=1.424601435661316)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0145], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.7095017433166504)\n",
       "            )\n",
       "          )\n",
       "          (relu): Identity()\n",
       "          (fc2): Conv2d(\n",
       "            168, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0103], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0655639171600342, max_val=1.3177181482315063)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0258], device='cuda:0'), zero_point=tensor([157], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.058445930480957, max_val=2.513153314590454)\n",
       "            )\n",
       "          )\n",
       "          (skip_mul): FloatFunctional(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([38], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3261573016643524, max_val=1.8778679370880127)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0110], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3973453044891357, max_val=1.1746692657470703)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0284], device='cuda:0'), zero_point=tensor([135], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.814514636993408, max_val=3.4163191318511963)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0097], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2381640672683716, max_val=0.8850787878036499)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0558], device='cuda:0'), zero_point=tensor([114], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.357395648956299, max_val=7.875267028808594)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0310], device='cuda:0'), zero_point=tensor([12], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3749590814113617, max_val=7.540613174438477)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False\n",
       "            (bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1007], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.628273963928223, max_val=12.839054107666016)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0952], device='cuda:0'), zero_point=tensor([93], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.811896324157715, max_val=15.466059684753418)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0589], device='cuda:0'), zero_point=tensor([6], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37477532029151917, max_val=14.649018287658691)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): QuantizableSqueezeExcitation(\n",
       "          (fc1): ConvReLU2d(\n",
       "            960, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0098], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0401265621185303, max_val=1.2455387115478516)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0150], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=3.828253746032715)\n",
       "            )\n",
       "          )\n",
       "          (relu): Identity()\n",
       "          (fc2): Conv2d(\n",
       "            240, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0157], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9376204609870911, max_val=2.0041520595550537)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0300], device='cuda:0'), zero_point=tensor([139], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.181153774261475, max_val=3.463186740875244)\n",
       "            )\n",
       "          )\n",
       "          (skip_mul): FloatFunctional(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0083], device='cuda:0'), zero_point=tensor([43], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35811373591423035, max_val=1.7659016847610474)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0134], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6908884048461914, max_val=1.704123616218567)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0221], device='cuda:0'), zero_point=tensor([127], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.809861421585083, max_val=2.815061092376709)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0342], device='cuda:0'), zero_point=tensor([131], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.469475269317627, max_val=4.25648307800293)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): QuantizableInvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5742940902709961, max_val=0.48017269372940063)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0553], device='cuda:0'), zero_point=tensor([132], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.289717674255371, max_val=6.802720546722412)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0271], device='cuda:0'), zero_point=tensor([14], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37495917081832886, max_val=6.5408549308776855)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False\n",
       "            (bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0952], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.134064674377441, max_val=10.378158569335938)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0877], device='cuda:0'), zero_point=tensor([101], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.859306335449219, max_val=13.493476867675781)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Hardswish(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0527], device='cuda:0'), zero_point=tensor([7], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3748953342437744, max_val=13.054937362670898)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): QuantizableSqueezeExcitation(\n",
       "          (fc1): ConvReLU2d(\n",
       "            960, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0101], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5935972332954407, max_val=1.289594054222107)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0237], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=6.055561065673828)\n",
       "            )\n",
       "          )\n",
       "          (relu): Identity()\n",
       "          (fc2): Conv2d(\n",
       "            240, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.648078203201294, max_val=1.0979706048965454)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0245], device='cuda:0'), zero_point=tensor([152], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.713703155517578, max_val=2.5363619327545166)\n",
       "            )\n",
       "          )\n",
       "          (skip_mul): FloatFunctional(\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0122], device='cuda:0'), zero_point=tensor([28], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.34140488505363464, max_val=2.7781410217285156)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ConvBNActivation(\n",
       "          (0): ConvBn2d(\n",
       "            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4693716764450073, max_val=1.635400414466858)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0520], device='cuda:0'), zero_point=tensor([129], device='cuda:0')\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.700862407684326, max_val=6.547613620758057)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0648], device='cuda:0'), zero_point=tensor([128], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.293062210083008, max_val=8.227482795715332)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): ConvBNActivation(\n",
       "      (0): ConvBn2d(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (weight_fake_quant): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0156], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9671446084976196, max_val=1.9915815591812134)\n",
       "        )\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1801], device='cuda:0'), zero_point=tensor([117], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-21.143613815307617, max_val=24.770065307617188)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): Hardswish(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0949], device='cuda:0'), zero_point=tensor([4], device='cuda:0')\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37465083599090576, max_val=23.828550338745117)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(\n",
       "      in_features=960, out_features=1280, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8469640612602234, max_val=0.7434737682342529)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0454], device='cuda:0'), zero_point=tensor([141], device='cuda:0')\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.38979434967041, max_val=5.185929775238037)\n",
       "      )\n",
       "    )\n",
       "    (1): Hardswish(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0211], device='cuda:0'), zero_point=tensor([18], device='cuda:0')\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37499168515205383, max_val=5.006286144256592)\n",
       "      )\n",
       "    )\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(\n",
       "      in_features=1280, out_features=1000, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0064], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4118260443210602, max_val=0.8177193999290466)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1198], device='cuda:0'), zero_point=tensor([81], device='cuda:0')\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.650811195373535, max_val=20.89242172241211)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0187], device='cuda:0'), zero_point=tensor([114], device='cuda:0')\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e31c68abf1d5dd3f9e2269f23eadf1b199587e56c0618a30760176a65ebfcab4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('lightweight': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
